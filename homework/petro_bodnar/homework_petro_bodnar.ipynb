{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T16:01:32.587379Z",
     "start_time": "2020-07-09T16:01:32.568746Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T16:40:32.942326Z",
     "start_time": "2020-07-09T16:40:32.900304Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing._encoders import _BaseEncoder\n",
    "import numpy as np\n",
    "\n",
    "class CardinalEncoder(_BaseEncoder):\n",
    "    \"\"\"\n",
    "    Encode categorical features as an integer array.\n",
    "    The only difference between this CardinalEncoder and the OrdinalEncoder\n",
    "    is unknown handling. OrdinalEncoder does not handle unknowns because\n",
    "    categories are presumed to have an order, and the ordinal position of\n",
    "    and unknown category cannot be inferred. Otherwise, these encoders function\n",
    "    the same way.\n",
    "    The input to this transformer should be an array-like of integers or\n",
    "    strings, denoting the values taken on by categorical (discrete) features.\n",
    "    The features are converted to cardinal integers. During transform, any \n",
    "    categories that are not recognized are grouped with a single integer label. \n",
    "    This results in a single column of integers (0 to n_categories) per feature.\n",
    "    Parameters\n",
    "    ----------\n",
    "    categories : 'auto' or a list of array-like, default='auto'\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories should not mix strings and numeric\n",
    "          values.\n",
    "        The used categories can be found in the ``categories_`` attribute.\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : boolean, default True\n",
    "        Whether or not to implement unknown handling. If False, unknown categories\n",
    "        will raise an error. If True, then CardinalEncoder will fit a copy of X\n",
    "        in which the final element is replaced with 'Unknown'. During transform,\n",
    "        the original form of X is encoded. **If the final element in X is unique**,\n",
    "        then no data loss will occur on a `fit_transform` call, but some data loss\n",
    "        may occur on subsequent `transforms` if the data includes instances of \n",
    "        the unique final fitted element as well as other unknown elements.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting\n",
    "        (in order of the features in X and corresponding with the output\n",
    "        of ``transform``).\n",
    "    \"\"\"\n",
    "    def __init__(self, categories='auto', dtype=np.float64, handle_unknown=True):\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the CardinalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to determine the categories of each feature.\n",
    "        y : None\n",
    "            Ignored. This parameter exists only for compatibility with\n",
    "            :class:`sklearn.pipeline.Pipeline`.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self._categories = self.categories # for compatibility with base encoder/OneHotEncoder's deprecations\n",
    "        \n",
    "        if self.handle_unknown:\n",
    "\n",
    "            if hasattr(X, \"iloc\"):\n",
    "                _X = X.copy()\n",
    "\n",
    "                if hasattr(X, \"name\"):\n",
    "                    _X = _X.to_frame()\n",
    "\n",
    "                _X.iloc[-1,:] = 'Unknown'\n",
    "\n",
    "            if isinstance(X, np.ndarray):\n",
    "                _X = np.copy(X)\n",
    "                if _X.ndim > 1:\n",
    "                    _X[-1, :] = 'Unknown'\n",
    "                else:\n",
    "                    _X[-1] = 'Unknown'\n",
    "        \n",
    "        self._fit(_X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform X to cardinal codes.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        if self.handle_unknown:\n",
    "            cats_ = self.categories_\n",
    "\n",
    "            if hasattr(X, 'iloc'):\n",
    "\n",
    "                if hasattr(X, 'name'):\n",
    "                    X = X.to_frame()\n",
    "\n",
    "                for i, col in enumerate(X.columns):\n",
    "                    X.loc[:,col] = X.loc[:, col].where(X.loc[:, col].isin(cats_[i]), 'Unknown')\n",
    "\n",
    "            if isinstance(X, np.ndarray):\n",
    "                if X.ndim > 1:\n",
    "                    for i in range(X.shape[1]):\n",
    "                        X_i = X[:,i]\n",
    "                        X_i[~np.isin(X[:,i], cats_[i])] = 'Unknown'\n",
    "                else:\n",
    "                    X[~np.isin(X, cats_[0])] = 'Unknown'\n",
    "\n",
    "        X_int, _ = self._transform(X)\n",
    "        return X_int.astype(self.dtype, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/31259891/put-customized-functions-in-sklearn-pipeline\n",
    "\n",
    "—Åustom elements of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T16:31:03.288413Z",
     "start_time": "2020-07-09T16:31:02.074967Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://s3.amazonaws.com/datarobot_public_datasets/DR_Demo_Fire_Ins_Loss_only.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets check if the types for different columns. I was not sure is there a direct way to implements type inference within sklearn pipe interface, therefore I decided to create a function which sorts types into different groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T11:11:18.867953Z",
     "start_time": "2020-07-09T11:11:18.834272Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Percentage of Unique: ',\n",
       "  3.2046014790468362,\n",
       "  'Mean number of symbols: ',\n",
       "  2.0,\n",
       "  'Col name: ',\n",
       "  'Rating_Class'),\n",
       " ('Percentage of Unique: ',\n",
       "  0.3286770747740345,\n",
       "  'Mean number of symbols: ',\n",
       "  1.0,\n",
       "  'Col name: ',\n",
       "  'Sub_Rating_Class'),\n",
       " ('Percentage of Unique: ',\n",
       "  0.5751848808545604,\n",
       "  'Mean number of symbols: ',\n",
       "  1.0,\n",
       "  'Col name: ',\n",
       "  'Renewal_class'),\n",
       " ('Percentage of Unique: ',\n",
       "  0.3286770747740345,\n",
       "  'Mean number of symbols: ',\n",
       "  1.0,\n",
       "  'Col name: ',\n",
       "  'Sub_Renewal_Class'),\n",
       " ('Percentage of Unique: ',\n",
       "  0.24650780608052586,\n",
       "  'Mean number of symbols: ',\n",
       "  1.0,\n",
       "  'Col name: ',\n",
       "  'Commercial'),\n",
       " ('Percentage of Unique: ',\n",
       "  0.16433853738701726,\n",
       "  'Mean number of symbols: ',\n",
       "  1.0,\n",
       "  'Col name: ',\n",
       "  'Renewal_Type'),\n",
       " ('Percentage of Unique: ',\n",
       "  0.4930156121610517,\n",
       "  'Mean number of symbols: ',\n",
       "  37.26869350862777,\n",
       "  'Col name: ',\n",
       "  'ISO_cat'),\n",
       " ('Percentage of Unique: ',\n",
       "  100.0,\n",
       "  'Mean number of symbols: ',\n",
       "  389.20049301561215,\n",
       "  'Col name: ',\n",
       "  'ISO_desc')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(\"Percentage of Unique: \", len(df[d].unique())/len(df) * 100,\n",
    "  \"Mean number of symbols: \", df[d].str.len().mean(),\n",
    "  \"Col name: \", df[d].name)\n",
    " for d in df.columns if df[d].dtype == 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've decided to use heuristic if unique values of column constitute more than 50% of all occurances and mean len of the column value if > 100 characters it should be considered as text. All other field with characters, should be considered a category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also consider some of the numeric with low rate of unique value to be categoric, but I will treat them as numeric for the sake of simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T11:15:31.624134Z",
     "start_time": "2020-07-09T11:15:31.599704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.6433853738701727, 'loss', dtype('float64')),\n",
       " (0.4930156121610517, 'Exposure', dtype('float64')),\n",
       " (0.7395234182415776, 'Property_size', dtype('float64')),\n",
       " (0.5751848808545604, 'Residents', dtype('float64')),\n",
       " (1.1503697617091209, 'Norm_fire_risk', dtype('float64')),\n",
       " (85.53820870994248, 'Norm_monthly_rent', dtype('float64')),\n",
       " (14.95480690221857, 'Loan_mortgage', dtype('float64')),\n",
       " (10.106820049301561, 'No_claim_Years', dtype('float64')),\n",
       " (1.2325390304026294, 'Previous_claims', dtype('float64')),\n",
       " (8.874281018898932, 'Norm_area_m', dtype('float64')),\n",
       " (2.3007395234182417, 'Premium_remain', dtype('float64')),\n",
       " (61.38044371405095, 'Premium_renew', dtype('float64')),\n",
       " (14.626129827444537, 'crime_property_type', dtype('float64')),\n",
       " (14.95480690221857, 'crime_residents', dtype('float64')),\n",
       " (12.818405916187345, 'crime_area', dtype('float64')),\n",
       " (14.872637633525063, 'crime_arson', dtype('float64')),\n",
       " (14.872637633525063, 'crime_burglary', dtype('float64')),\n",
       " (7.8882497945768275, 'crime_neighbour_watch', dtype('float64')),\n",
       " (14.708299096138045, 'crime_community', dtype('float64')),\n",
       " (15.036976170912078, 'crime_risk', dtype('float64')),\n",
       " (39.60558751027116, 'Geographical_risk', dtype('float64')),\n",
       " (74.03451109285128, 'Weather_risk', dtype('float64')),\n",
       " (0.4930156121610517, 'ISO', dtype('int64'))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(len(df[d].unique())/len(df) * 100, df[d].name, df[d].dtype) for d in df.columns if df[d].dtype != 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T11:20:31.837574Z",
     "start_time": "2020-07-09T11:20:31.820637Z"
    }
   },
   "outputs": [],
   "source": [
    "def type_inference(data):\n",
    "    categoric = []\n",
    "    numberic = []\n",
    "    text = []\n",
    "    for d in data.columns:\n",
    "        if data[d].dtype == 'object':\n",
    "            if ((len(data[d].unique())/len(data)) > 0.5) and (data[d].str.len().mean() > 100):\n",
    "                text.append(d)\n",
    "            else:\n",
    "                categoric.append(d)\n",
    "        else:\n",
    "            numberic.append(d)\n",
    "    return numberic, categoric, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T11:41:32.134716Z",
     "start_time": "2020-07-09T11:41:32.124039Z"
    }
   },
   "outputs": [],
   "source": [
    "target_feature = 'crime_risk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature enginering and building pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't found obvious options for feature enginering, therefore I decided to use feature selection as as part of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:18:25.010803Z",
     "start_time": "2020-07-09T15:18:25.003038Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:18:25.590275Z",
     "start_time": "2020-07-09T15:18:25.586064Z"
    }
   },
   "outputs": [],
   "source": [
    "target_feature = 'Norm_monthly_rent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:18:26.168217Z",
     "start_time": "2020-07-09T15:18:26.164004Z"
    }
   },
   "outputs": [],
   "source": [
    "y = data.pop(target_feature)\n",
    "X = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:18:26.712275Z",
     "start_time": "2020-07-09T15:18:26.703553Z"
    }
   },
   "outputs": [],
   "source": [
    "numberci, categoric, text = type_inference(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:18:27.239637Z",
     "start_time": "2020-07-09T15:18:27.222823Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "# y should be an ndarray\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "class StackingTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"A transformer that wraps a RegressorMixin `est` predict function.\n",
    "    \n",
    "    It overloads `fit_transform` to do a stacked transform to avoid leakage.\n",
    "    \"\"\"\n",
    "    def __init__(self, est):\n",
    "        self.est = est\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if y is None:\n",
    "            raise ValueError('{}.fit requires y to be not None'.format(self))\n",
    "        self.est.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.est.predict(X)[:, np.newaxis]\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if y is None:\n",
    "            raise ValueError('{}.fit requires y to be not None'.format(self))\n",
    "        self.models_ = []\n",
    "        out = np.empty_like(y)[:, np.newaxis]\n",
    "        for train, test in KFold(5, shuffle=True, random_state=0).split(X):\n",
    "            self.models_.append(clone(self.est).fit(X[train], y[train]))\n",
    "            out[test, 0] = self.models_[-1].predict(X[test])\n",
    "        self.est = self.models_[0]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T16:36:21.865950Z",
     "start_time": "2020-07-09T16:36:21.861192Z"
    }
   },
   "source": [
    "Looks that Ordinal Encoder doesn't know how to treat unknown values. <br>\n",
    "I've found this [discussion](https://github.com/scikit-learn/scikit-learn/issues/13488) noting that it would just throw errors when finding unknown values. Therefore I decided to use custom Categorical Encoder from [this repo](https://github.com/jdraines/cardinal_encoder/blob/master/cardinal_encoder.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T16:43:30.442744Z",
     "start_time": "2020-07-09T16:43:30.416504Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=999999)),\n",
    "    ])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=\"missing_value\")),\n",
    "        ('ordinal', CardinalEncoder()),\n",
    "        ('select_feature', SelectFromModel(RandomForestRegressor(), threshold=None))\n",
    "    ])\n",
    "\n",
    "numeric_pipeline_linear = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ])\n",
    "\n",
    "categorical_pipeline_linear = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=\"missing_value\")),\n",
    "        ('one_hot', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('select_feature', SelectFromModel(ElasticNet(alpha=0.1), threshold=None))\n",
    "    ])\n",
    "\n",
    "text_pipeline = Pipeline(steps=[\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('bow_reg', StackingTransformer(ElasticNet(alpha=0.1)))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numberci),\n",
    "        ('cat', categorical_pipeline, categoric),\n",
    "        ('text', text_pipeline, text[0])\n",
    "\n",
    "    ])\n",
    "\n",
    "linear_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline_linear, numberci),\n",
    "        ('cat', categorical_pipeline_linear, categoric),\n",
    "        ('text', text_pipeline, text[0])\n",
    "\n",
    "    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T16:24:34.843642Z",
     "start_time": "2020-07-09T16:24:34.837441Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:47:28.190185Z",
     "start_time": "2020-07-09T15:47:28.179180Z"
    }
   },
   "outputs": [],
   "source": [
    "regressor = [\n",
    "    AdaBoostRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:43:15.245136Z",
     "start_time": "2020-07-09T15:43:15.237563Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'reg__n_estimators': [5,10, 15],\n",
    "    'reg__criterion': ['mae', 'friedman_mse']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure if it does make a lot of sence to combine model selection step with hyperparameter optimization. However, it is interesting that I can do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T15:55:07.906332Z",
     "start_time": "2020-07-09T15:53:55.053223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg__n_estimators': 5}\n",
      "model score: 0.5369803792107881\n",
      "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "                  n_estimators=50, random_state=None)\n",
      "{'reg__criterion': 'mae', 'reg__n_estimators': 5}\n",
      "model score: 0.6334707911347927\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False)\n",
      "{'reg__criterion': 'friedman_mse', 'reg__n_estimators': 10}\n",
      "model score: 0.520816901153717\n",
      "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
      "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                          n_iter_no_change=None, presort='deprecated',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "for reg in regressor:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('reg', reg)])\n",
    "    \n",
    "    suitable_grid  = {d: param_grid.get(d) for d in param_grid if d.split(\"__\")[1]\n",
    "                      in [dd for dd in reg.get_params().keys()]}\n",
    "    \n",
    "    CV = GridSearchCV(pipe, suitable_grid, n_jobs= 1)\n",
    "    \n",
    "    CV.fit(X_train, y_train)  \n",
    "    print(CV.best_params_)    \n",
    "    print(f\"model score: {CV.best_score_}\")\n",
    "    \n",
    "    print(reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it would be much quicker with one model, therefore we can check some parameters while doing model selection and than do the in depth fine tuning with the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T16:51:06.811505Z",
     "start_time": "2020-07-09T16:51:05.219682Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 769.254\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "est = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                  ('rf', RandomForestRegressor(n_estimators=5, criterion='mae', n_jobs=-1))])\n",
    "\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "print(\"model score: %.3f\" % mean_absolute_error(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T16:51:14.886298Z",
     "start_time": "2020-07-09T16:51:13.323653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 913.507\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "est = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                  ('rf', GradientBoostingRegressor(n_estimators=5))])\n",
    "\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "print(\"model score: %.3f\" % mean_absolute_error(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T16:51:20.133077Z",
     "start_time": "2020-07-09T16:51:18.820934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 761.420\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "est = Pipeline(steps=[('preprocessor', linear_preprocessor),\n",
    "                  ('rf', LinearRegression())])\n",
    "\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "print(\"model score: %.3f\" % mean_absolute_error(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we had similar results for RF and Linear Regression on this task working on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
